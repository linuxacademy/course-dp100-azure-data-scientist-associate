{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run a Batch Inferencing Pipeline and Obtain Outputs\n",
        "\n",
        "In this hands-on lab scenario you are a Data Scientist for Awesome Company. Recently the company has been using Azure Machine Learning Service to implement and run their machine learning workloads. One of the problems they are addressing is the need to predict diabetes cases based off of large numbers of patient records. In order to accomplish this you are going to implement a batch inferencing pipeline.\n",
        "\n",
        "To accomplish your goal, the following should be completed:\n",
        "* Train and register a model\n",
        "* Simulate sample data\n",
        "* Create and run a batch inferencing pipeline\n",
        "* Obtain the outputs of the experiment\n",
        "\n",
        "*This example notebook is adopted from openly available Microsoft Learn material.*"
      ],
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to your workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "\n",
        "# Load the workspace from the saved config file\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621531429714
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and register a model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\n",
        "from azureml.core import Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Create an Azure ML experiment in your workspace\n",
        "experiment = Experiment(workspace=ws, name='mslearn-train-diabetes')\n",
        "run = experiment.start_logging()\n",
        "print(\"Starting experiment:\", experiment.name)\n",
        "\n",
        "# load the diabetes dataset\n",
        "print(\"Loading Data...\")\n",
        "diabetes = pd.read_csv('data/diabetes.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
        "\n",
        "# Split data into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "\n",
        "# Train a decision tree model\n",
        "print('Training a decision tree model')\n",
        "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "\n",
        "# calculate accuracy\n",
        "y_hat = model.predict(X_test)\n",
        "acc = np.average(y_hat == y_test)\n",
        "print('Accuracy:', acc)\n",
        "run.log('Accuracy', float(acc))\n",
        "\n",
        "# calculate AUC\n",
        "y_scores = model.predict_proba(X_test)\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "print('AUC: ' + str(auc))\n",
        "run.log('AUC', float(auc))\n",
        "\n",
        "# Save the trained model\n",
        "model_file = 'diabetes_model.pkl'\n",
        "joblib.dump(value=model, filename=model_file)\n",
        "run.upload_file(name = 'outputs/' + model_file, path_or_stream = './' + model_file)\n",
        "\n",
        "# Complete the run\n",
        "run.complete()\n",
        "\n",
        "# Register the model\n",
        "run.register_model(model_path='outputs/diabetes_model.pkl', model_name='diabetes_model',\n",
        "                   tags={'Training context':'Inline Training'},\n",
        "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\n",
        "\n",
        "print('Model trained and registered.')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621531556589
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulate Sample Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Datastore, Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Set default data store\n",
        "ws.set_default_datastore('workspaceblobstore')\n",
        "default_ds = ws.get_default_datastore()\n",
        "\n",
        "# Enumerate all datastores, indicating which is the default\n",
        "for ds_name in ws.datastores:\n",
        "    print(ds_name, \"- Default =\", ds_name == default_ds.name)\n",
        "\n",
        "# Load the diabetes data\n",
        "diabetes = pd.read_csv('data/diabetes2.csv')\n",
        "# Get a 100-item sample of the feature columns (not the diabetic label)\n",
        "sample = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].sample(n=100).values\n",
        "\n",
        "# Create a folder\n",
        "batch_folder = './batch-data'\n",
        "os.makedirs(batch_folder, exist_ok=True)\n",
        "print(\"Folder created!\")\n",
        "\n",
        "# Save each sample as a separate file\n",
        "print(\"Saving files...\")\n",
        "for i in range(100):\n",
        "    fname = str(i+1) + '.csv'\n",
        "    sample[i].tofile(os.path.join(batch_folder, fname), sep=\",\")\n",
        "print(\"files saved!\")\n",
        "\n",
        "# Upload the files to the default datastore\n",
        "print(\"Uploading files to datastore...\")\n",
        "default_ds = ws.get_default_datastore()\n",
        "default_ds.upload(src_dir=\"batch-data\", target_path=\"batch-data\", overwrite=True, show_progress=True)\n",
        "\n",
        "# Register a dataset for the input data\n",
        "batch_data_set = Dataset.File.from_files(path=(default_ds, 'batch-data/'), validate=False)\n",
        "try:\n",
        "    batch_data_set = batch_data_set.register(workspace=ws, \n",
        "                                             name='batch-data',\n",
        "                                             description='batch data',\n",
        "                                             create_new_version=True)\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "\n",
        "print(\"Done!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621531821582
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assign the compute target to the compute *cluster* already prepared for you"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "cluster_name = \"acaml-****\"\n",
        "\n",
        "try:\n",
        "    # Check for existing compute target\n",
        "    inference_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    ws = ws.update(image_build_compute=inference_cluster),
        "    print('Found existing cluster and using it.')\n",
        "except ComputeTargetException:\n",
        "    # If it doesn't already exist, say so\n",
        "    print('There is no existing cluster by that name.')\n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621532371244
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a pipeline for batch inferencing"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an experiment folder"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Create a folder for the experiment files\n",
        "experiment_folder = 'batch_pipeline'\n",
        "os.makedirs(experiment_folder, exist_ok=True)\n",
        "\n",
        "print(experiment_folder)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621532380228
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Python script and save it in the pipeline folder"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/batch_diabetes.py\n",
        "import os\n",
        "import numpy as np\n",
        "from azureml.core import Model\n",
        "import joblib\n",
        "\n",
        "\n",
        "def init():\n",
        "    # Runs when the pipeline step is initialized\n",
        "    global model\n",
        "\n",
        "    # load the model\n",
        "    model_path = Model.get_model_path('diabetes_model')\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "\n",
        "def run(mini_batch):\n",
        "    # This runs for each batch\n",
        "    resultList = []\n",
        "\n",
        "    # process each file in the batch\n",
        "    for f in mini_batch:\n",
        "        # Read the comma-delimited data into an array\n",
        "        data = np.genfromtxt(f, delimiter=',')\n",
        "        # Reshape into a 2-dimensional array for prediction (model expects multiple items)\n",
        "        prediction = model.predict(data.reshape(1, -1))\n",
        "        # Append prediction to results\n",
        "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\n",
        "    return resultList"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a run context with the needed dependencies"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
        "from azureml.core.runconfig import CondaDependencies\n",
        "\n",
        "# Add dependencies required by the model\n",
        "# For scikit-learn models, you need scikit-learn\n",
        "# For parallel pipeline steps, you need azureml-core and azureml-dataprep[fuse]\n",
        "cd = CondaDependencies.create(conda_packages=['scikit-learn=1.1.3','pip'],\n",
        "                              pip_packages=['azureml-defaults','azureml-core','azureml-dataprep[fuse]'])\n",
        "\n",
        "batch_env = Environment(name='batch_environment')\n",
        "batch_env.python.conda_dependencies = cd\n",
        "batch_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
        "print('Configuration ready.')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621532385793
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure a **ParallelRunStep**\n",
        "\n",
        "Note: An *'enabled' is deprecated* warning may be displayed - you can ignore this."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
        "from azureml.pipeline.core import PipelineData\n",
        "from azureml.core.runconfig import DockerConfiguration\n",
        "\n",
        "default_ds = ws.get_default_datastore()\n",
        "\n",
        "output_dir = PipelineData(name='inferences', \n",
        "                          datastore=default_ds, \n",
        "                          output_path_on_compute='diabetes/results')\n",
        "\n",
        "parallel_run_config = ParallelRunConfig(\n",
        "    source_directory=experiment_folder,\n",
        "    entry_script=\"batch_diabetes.py\",\n",
        "    mini_batch_size=\"5\",\n",
        "    error_threshold=10,\n",
        "    output_action=\"append_row\",\n",
        "    environment=batch_env,\n",
        "    compute_target=inference_cluster,\n",
        "    node_count=2)\n",
        "\n",
        "parallelrun_step = ParallelRunStep(\n",
        "    name='batch-score-diabetes',\n",
        "    parallel_run_config=parallel_run_config,\n",
        "    inputs=[batch_data_set.as_named_input('diabetes_batch')],\n",
        "    output=output_dir,\n",
        "    arguments=[],\n",
        "    allow_reuse=True\n",
        ")\n",
        "\n",
        "print('Steps defined')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621532390520
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the pipeline from the defined steps and run\n",
        "\n",
        "Note: This may take some time. Make sure you click on the link displayed on the returned output, right below the code cell, to more easily follow the progress of the pipeline. Then, you can come back here to run the next code cell to view outputs."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\n",
        "from azureml.pipeline.core import Pipeline\n",
        "\n",
        "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
        "pipeline_run = Experiment(ws, 'mslearn-diabetes-batch').submit(pipeline)\n",
        "pipeline_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "gather": {
          "logged": 1621533182313
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve the saved outputs"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# Remove the local results folder if left over from a previous run\n",
        "shutil.rmtree('diabetes-results', ignore_errors=True)\n",
        "\n",
        "# Get the run for the first step and download its output\n",
        "prediction_run = next(pipeline_run.get_children())\n",
        "prediction_output = prediction_run.get_output_data('inferences')\n",
        "prediction_output.download(local_path='diabetes-results')\n",
        "\n",
        "# Traverse the folder hierarchy and find the results file\n",
        "for root, dirs, files in os.walk('diabetes-results'):\n",
        "    for file in files:\n",
        "        if file.endswith('parallel_run_step.txt'):\n",
        "            result_file = os.path.join(root,file)\n",
        "\n",
        "# cleanup output format\n",
        "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
        "df.columns = [\"File\", \"Prediction\"]\n",
        "\n",
        "# Display the first 20 results\n",
        "df.head(20)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621533247842
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
